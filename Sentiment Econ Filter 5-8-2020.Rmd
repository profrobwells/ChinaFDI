---
title: "Sentiment Analysis May 8 2020"
author: "Rob Wells"
date: "6/22/2020"
output: html_document
---

# Sentiment Analysis of Economic Filter corpus of articles, 5,306 articles from 2000-2019

Code is adapted from EF_Sentiment-local.R

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "")
getwd()
```

Load libraries
```{r include=FALSE}
#install.packages("sentimentr")
#install.packages("tidytext")
#install.packages("textdata")
#install.packages("rio")
library(textdata)
library(sentimentr)
library(tidytext)
library(rio)
library(tidyverse)
```
### Import
```{r}
EF <- rio::import("./EconomicFilter/Economicfilter2020.csv")
```

### Clean stray HTML from Text field
```{r}
# Using the rvest library
# html_text() will remove all html tags
# read_html() uses XML2 to put the text into a format the html_text can easily parse
# These are put into a function to map to each row in the dataframe
library(rvest)
library(purrr)
strip_html <- function(x) {
    html_text(read_html(x))
}

EF$Text <- map(EF$Text,strip_html)
EF$Text <- as.character(EF$Text)
rm(strip_html)
```




### Tokenize the TEXT table)
```{r}
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
TEXT_token <- EF %>%
  filter(!str_detect(Text, '^"')) %>%
  #mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#Clean out ' character in word columm of TEXT_token
#example: 'growing
TEXT_token$word <- 
  gsub("'","", TEXT_token$word)

#remove spaces
TEXT_token$word <- 
  gsub(" ","", TEXT_token$word)


#TEXT_token <- select (TEXT_token, c(headline, article_nmbr, linenumber, word2))
#colnames(TEXT_token)[4] <- "word"

#Table of Common Words 
CommonTEXTWords <- TEXT_token %>%
  count(word) %>%
  filter(sum(n) >= 5) %>%
  arrange(desc(n))


#save
write.csv(CommonTEXTWords, "./EconomicFilter/EF_Words_Total.csv")
```

### Table of Common Words by Publication
```{r}
CommonTEXTWords2 <- TEXT_token %>%
  group_by(Pub) %>% 
  count(word) %>%
  filter(sum(n) >= 5) %>%
  arrange(desc(n))
```

### Date Common Words by Publication
```{r}
DateWords <- TEXT_token %>%
  mutate(year =year(Date))
  
  YearWords <- DateWords %>% 
  select(year, Pub, word) %>% 
  group_by(year) %>% 
  count(word) %>%
  filter(n >= 5) %>%
  arrange(desc(n))
  
  #save
write.csv(YearWords, "./EconomicFilter/YearWords.csv")
```




Load dictionaries

```{r}
afinn <-get_sentiments("afinn")
bing <- get_sentiments("bing")
```

```{r}
#Have to load NRC separately
#http://sentiment.nrc.ca/lexicons-for-research/
# nrc <- rio::import("/Users/robwells/Dropbox/Current_Projects/China Notes  Background/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/Older Versions/NRC-Emotion-Lexicon-v0.92-InManyLanguages.xlsx")
# nrc <- janitor::clean_names(nrc)
# nrc <- nrc %>% 
#   select(english_word, positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust)
# colnames(nrc)[1] <- "word"
# write.csv(nrc, "nrc.csv")
# 
# nrc %>% 
#   count(positive, negative)
# 
# nrc1 <- rio::import("/Users/robwells/Dropbox/Current_Projects/China Notes  Background/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt")
# colnames(nrc1)[1:3] <- c("word", "sentiment", "score")
# write.csv(nrc1, "nrc1.csv")
# #
```

```{r}
setwd("~/Dropbox/Current_Projects/ChinaFDI")
nrc1 <- rio::import("nrc1.csv")

nrc2 <- nrc1 %>% 
  filter(score >=1)
```

# Load Economic Filter data

```{r}
setwd("~/Dropbox/Current_Projects/ChinaFDI")
YearWords <- rio::import("./EconomicFilter/YearWords.csv")
```
# Filter noise like President, Trump,  Premier

```{r}
junk <- c("trump", "president", "china", "chinese", "american", "trumps", "canada", "european", "chinas", 
          "beijing", "north", "south", "korea", "mexico", "washington", "mstar", "united", "states", "government",
          "administration", "white", "house", "xi", "premier", "lighthizer", "wto", "issue", "issues", "vice")
```

```{r}
YearWords <- YearWords %>% 
  filter(!word  %in% junk)
```

### Preferred method of calculating sentiment: Sentiment * Word Frequency
```{r}
total_BINGYearsentiment <- YearWords  %>%
  inner_join(bing, by = "word") %>% 
   select(year, word, sentiment, n) 
head(total_BINGYearsentiment)
```


```{r}
sum(total_BINGYearsentiment$n)
```

```{r}
x <- total_BINGYearsentiment
```

### Transforming the 'negative' and 'positive' into a numeric value
```{r}
x$score  <- x$sentiment

x$score <- gsub("positive", "1", x$score)
x$score <- gsub("negative", "-1", x$score)
x$score <- as.numeric(x$score)
glimpse(x)
```


```{r}
x<- x %>% 
  mutate(newscore = (score*n))
head(x)
```         

```{r}
bing_by_year_EF <- x %>%
   group_by(year, sentiment) %>% 
     summarise(yrtotal = sum(newscore))
```

-- Total Score

```{r}
average_bing_by_year_EF <- x %>%
   group_by(year, sentiment, newscore) %>% 
     summarise(yrtotal = sum(newscore)) %>% 
    summarise(yr_average = mean(newscore))  
head(average_bing_by_year_EF)
```

-- Average Postive / Negative Score

```{r}
bing_AVG_EF_sent_year <- x %>% 
   group_by(year, sentiment) %>% 
  summarise(yr_average = mean(newscore))  
head(bing_AVG_EF_sent_year)
```



```{r}
#write.csv(bing_by_year_EF, "EF_bing_year_total.csv")
#write.csv(bing_AVG_EF_sent_year, "bing_AVG_EF_sent_year.csv")


```

### Combined average sentiment score
```{r}
xx <- x %>%
   group_by(year) %>% 
     summarise(EF_yravg = mean(newscore))
```

```{r}
write.csv(xx, "./Sentiment Analysis/EF_single_bing_year_6_22_2020.csv")
head(xx)
```

```{r}
mean(xx$EF_yravg)
```

```{r}
xx %>% 
ggplot(aes(x= year, y= EF_yravg, color = EF_yravg > -1.011729, size = 4)) +
  geom_point() +
  scale_x_continuous(breaks=c(2000:2019)) +
  theme(legend.position = "none") +
   theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   labs(title = "China News Sentiment by Year", 
       subtitle = "Bing Average Sentiment, Economic News",
       caption = "Blue= Above average sentiment of -2.1. Red = Below average sentiment. \n Source: ProQuest - Economic Filter Search. 5,306 articles
       Graphic by Rob Wells. 6-22-2020",
       x=" ",
       y="Sentiment: Negative -> Positive") 

#ggsave("AvgChina_Sentiment.png",device = "png",width=10,height=8, dpi=800)

```

# Custom Sentiment Analysis

-- This script comes from Custom Sentiment list 7-5-19.R, and a May 25 script.

bing <- get_sentiments("bing")
#bing columns are word and sentiment
afinn <- get_sentiments("afinn")
#afinn columns are word and score

### Importing custom list of 176 trade terms curated by Wells and Zeng. 
--  Full list is in Box, Negative-Postiive Key Terms on Trade 4-18-19.xlsx
-- trade_terms columns are word and score
```{r}
trade_terms <- rio::import("./Sentiment Analysis/trade_sentiment_5-25.csv")
head(trade_terms)
```

```{r}
head(YearWords)
```
## Total Custom Sentiment for EF

### Preferred method of calculating sentiment: Sentiment * Word Frequency
```{r}
total_CustomYearsentiment <- YearWords  %>%
  inner_join(trade_terms, by = "word") %>% 
   select(year, word, rating, n) 
head(total_CustomYearsentiment)
```


```{r}
sum(total_CustomYearsentiment$n)
```

```{r}
a <- total_CustomYearsentiment
```


```{r}
a<- a %>% 
  mutate(newscore = (rating*n))
head(a)
```         



### Table of custom sentiment score by year for Economic Filter
## Insert average here

### Average custom sentiment score
```{r}
custom_by_year_EF <- a %>%
   group_by(year) %>% 
     summarise(EFyravg = mean(newscore))
head(custom_by_year_EF)
write.csv(custom_by_year_EF, "./Sentiment Analysis/custom_by_year_EF.csv")
```


```{r}
#Average custom sentiment score is -7.80097
mean(custom_by_year_EF$EFyravg)
```

```{r}
custom_by_year_EF %>% 
ggplot(aes(x= year, y= EFyravg, color = EFyravg > -7.80097, size = 4)) +
  geom_point() +
  scale_x_continuous(breaks=c(2000:2019)) +
  theme(legend.position = "none") +
   theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   labs(title = "Custom Sentiment: China Economic News", 
       subtitle = "Custom Sentiment Score, Chinese Investment in U.S.",
       caption = "Blue= Above average sentiment of -7.8. Red = Below average sentiment. \n Source: ProQuest - Economic Filter Search. 5,306 articles
       Graphic by Rob Wells. 5-17-2020",
       x=" ",
       y="Sentiment: Negative -> Positive") 

ggsave("CustomAvgChina_Sentiment.png",device = "png",width=10,height=8, dpi=800)

```
